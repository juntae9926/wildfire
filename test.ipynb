{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "MAX_EPOCHS = 5\n",
    "\n",
    "def seed_everything(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    print(f\"Seed locked: {random_seed}\")\n",
    "\n",
    "seed_everything(random_seed=42)\n",
    "\n",
    "# Load the dataset from the CSV file\n",
    "# data = pd.read_csv('data_230817.csv', header=None)\n",
    "# data = pd.read_csv('data_230916.csv', header=None, skiprows=1, index_col=0)\n",
    "# data.columns = [\n",
    "#     '진입 탈출로 개수',\n",
    "#     '30m 이내 진입 탈출로 개수',\n",
    "#     '최소 도로폭',\n",
    "#     '교차로 개수',\n",
    "#     '침엽수림 비율',\n",
    "#     '산림기준 이격 거리',\n",
    "#     '창문',\n",
    "#     '산불 진화용수 거리',\n",
    "#     '소방서와의 거리',\n",
    "#     '주건물 지붕특성',\n",
    "#     '불난 후 상태'\n",
    "# ]\n",
    "\n",
    "\n",
    "data_general = pd.read_csv('data/data_general.csv', header=None, skiprows=1, index_col=0)\n",
    "data_mountain = pd.read_csv('data/data_mountain.csv', header=None, skiprows=1, index_col=0)\n",
    "data = pd.concat([data_general, data_mountain])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data.columns = [\n",
    "    '진입 탈출로 개수',\n",
    "    '30m 이내 진입 탈출로 개수',\n",
    "    '최소 도로폭',\n",
    "    '교차로 개수',\n",
    "    '침엽수림 비율',\n",
    "    '산림기준 이격 거리',\n",
    "    '창문',\n",
    "    '산불 진화용수 거리',\n",
    "    '소방서와의 거리',\n",
    "    '담 유무',\n",
    "    '산림방향 담 유무',\n",
    "    '최대 경사도',\n",
    "    '최소 경사도',\n",
    "    '주건물 지붕특성',\n",
    "    '비산거리',\n",
    "    '불난 후 상태'\n",
    "]\n",
    "data = data.drop(['진입 탈출로 개수', '담 유무', '산림방향 담 유무', '최대 경사도', '최소 경사도'], axis=1)\n",
    "# data = data.drop(['교차로 개수', '산불 진화용수 거리', '소방서와의 거리', '담 유무', '산림방향 담 유무', '최대 경사도', '최소 경사도'], axis=1)\n",
    "\n",
    "# 마을 별 구분\n",
    "# indices = data.index[data.isna().any(axis=1)].tolist()\n",
    "indices = data.index[data['불난 후 상태'].isna()].tolist()\n",
    "data_blocks = []\n",
    "start = 0\n",
    "for idx in indices:\n",
    "    if idx == 0:\n",
    "        continue\n",
    "    data_blocks.append(data.iloc[start:idx])\n",
    "    start = idx + 1\n",
    "\n",
    "if start < len(data):\n",
    "    data_blocks.append(data.iloc[start:])\n",
    "\n",
    "data_blocks = [block.dropna() for block in data_blocks]\n",
    "\n",
    "# 9,10 마을 제외\n",
    "# a = data_blocks.pop(-1)\n",
    "# b = data_blocks.pop(-1)\n",
    "# data_blocks = []\n",
    "# data_blocks.append(a)\n",
    "# data_blocks.append(b)\n",
    "\n",
    "data_blocks[9]['산불 진화용수 거리'] = data_blocks[9]['산불 진화용수 거리'] / 1000\n",
    "data_blocks[9]['산불 진화용수 거리'] = data_blocks[9]['산불 진화용수 거리'].apply(lambda x: round(x, 3))\n",
    "\n",
    "data_blocks[10]['산불 진화용수 거리'] = data_blocks[10]['산불 진화용수 거리'] / 1000\n",
    "data_blocks[10]['산불 진화용수 거리'] = data_blocks[10]['산불 진화용수 거리'].apply(lambda x: round(x, 3))\n",
    "\n",
    "data_blocks[9]['소방서와의 거리'] = data_blocks[9]['소방서와의 거리'] / 1000\n",
    "data_blocks[9]['소방서와의 거리'] = data_blocks[9]['소방서와의 거리'].apply(lambda x: round(x, 3))\n",
    "\n",
    "data_blocks[10]['소방서와의 거리'] = data_blocks[10]['소방서와의 거리'] / 1000\n",
    "data_blocks[10]['소방서와의 거리'] = data_blocks[10]['소방서와의 거리'].apply(lambda x: round(x, 3))\n",
    "\n",
    "data_len = [len(block) for block in data_blocks]\n",
    "print(data_len)\n",
    "\n",
    "# 산림 기준 학습 split\n",
    "# split = [64.17, 38.0, 35.52, 53.92, 57.39, 67.93, 83.0, 22.67, 23.27, 34.0, 32.63]\n",
    "# over = [split.index(x) for x in split if x > 50.0]\n",
    "# under = [split.index(x) for x in split if x < 50.0]\n",
    "\n",
    "# over\n",
    "# data_blocks = [data_blocks[i] for i in over]\n",
    "# data_blocks.pop(4)\n",
    "# data_blocks.pop(6)\n",
    "# data_blocks.pop(6)\n",
    "# data_len = [len(block) for block in data_blocks]\n",
    "# print(data_len)\n",
    "\n",
    "# o = data_blocks.pop(0)\n",
    "# a = data_blocks.pop(3)\n",
    "# b = data_blocks.pop(5)\n",
    "# c = data_blocks.pop(5)\n",
    "# data_blocks = []\n",
    "# data_blocks.append(o)\n",
    "# data_blocks.append(a)\n",
    "# data_blocks.append(b)\n",
    "# data_blocks.append(c)\n",
    "\n",
    "# data_len = [len(block) for block in data_blocks]\n",
    "# print(data_len)\n",
    "\n",
    "\n",
    "# Random set 구분\n",
    "# data = pd.concat(data_blocks).reset_index()\n",
    "# if \"Set\" not in data.columns:\n",
    "    # data[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(data.shape[0]))\n",
    "\n",
    "\n",
    "# validation 데이터를 전체의 10퍼센트씩으로 구성할 경우\n",
    "vals = []\n",
    "for idx in range(len(data_blocks)):\n",
    "    val = data_blocks[idx].sample(frac=0.10)\n",
    "    data_blocks[idx].drop(val.index, inplace=True)\n",
    "    vals.append(val)\n",
    "val_block = pd.concat(vals)\n",
    "data_len = [len(block) for block in data_blocks]\n",
    "print(data_len)\n",
    "\n",
    "\n",
    "# 학습\n",
    "test_accs = []\n",
    "weights = []\n",
    "for i in range(len(data_blocks)):\n",
    "    # if not i > 8:\n",
    "    #     continue\n",
    "\n",
    "    save_dir = f'figs/{i}/'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    _data_blocks = data_blocks.copy()\n",
    "    # val_block = data_blocks.pop(0)\n",
    "    test_block = _data_blocks.pop(i)\n",
    "    \n",
    "    train_blocks = pd.concat(_data_blocks)\n",
    "    print(f\"lenths of train: {len(train_blocks)}  |  val: {len(val_block)}  |  test: {len(test_block)} | total: {len(train_blocks)+len(val_block)+len(test_block)}\")\n",
    "    val_block[\"Set\"] = 'valid'\n",
    "    test_block[\"Set\"] = 'test'\n",
    "    train_blocks[\"Set\"] = 'train'\n",
    "    data = pd.concat([val_block, test_block, train_blocks]).reset_index()\n",
    "    \n",
    "\n",
    "    train_indices = data[data.Set==\"train\"].index\n",
    "    valid_indices = data[data.Set==\"valid\"].index\n",
    "    test_indices = data[data.Set==\"test\"].index\n",
    "\n",
    "    nunique = data.nunique()\n",
    "    types = data.dtypes\n",
    "\n",
    "    categorical_columns = []\n",
    "    categorical_dims = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        if types[col] == 'object' or nunique[col] < 20:\n",
    "            # print(col, data[col].nunique())\n",
    "            l_enc = LabelEncoder()\n",
    "            data[col] = data[col].fillna(0.0)\n",
    "            data[col] = l_enc.fit_transform(data[col].values)\n",
    "            categorical_columns.append(col)\n",
    "            categorical_dims[col] = len(l_enc.classes_)\n",
    "        \n",
    "        else:\n",
    "            data.fillna(data.loc[train_indices, col].mean(), inplace=True)\n",
    "\n",
    "    data.drop(['index'], axis=1, inplace=True)\n",
    "    target = '불난 후 상태'\n",
    "    unused_feat = ['Set']\n",
    "    features = [col for col in data.columns if col not in unused_feat+[target]]\n",
    "    cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "    cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "    X_train = data[features].values[train_indices]\n",
    "    y_train = data[target].values[train_indices]\n",
    "\n",
    "    X_valid = data[features].values[valid_indices]\n",
    "    y_valid = data[target].values[valid_indices]\n",
    "\n",
    "    X_test = data[features].values[test_indices]\n",
    "    y_test = data[target].values[test_indices]\n",
    "\n",
    "    ## Train\n",
    "    clf = TabNetClassifier(cat_idxs=cat_idxs,\n",
    "                        cat_dims=cat_dims,\n",
    "                        cat_emb_dim=64,\n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=2e-2),\n",
    "                        scheduler_params={\"step_size\":1,\n",
    "                                            \"gamma\":0.99},\n",
    "                        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                        mask_type='entmax' # \"sparsemax\", entmax)\n",
    "                        )\n",
    "\n",
    "\n",
    "    save_history = []\n",
    "    clf.fit(\n",
    "            X_train=X_train, y_train=y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=['accuracy'],\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            patience=20,\n",
    "            batch_size=256,\n",
    "            virtual_batch_size=256,\n",
    "            num_workers=12,\n",
    "            weights=1,\n",
    "            drop_last=False\n",
    "            )\n",
    "    save_history.append(clf.history[\"valid_accuracy\"])\n",
    "    clf.save_model('./ckpt')\n",
    "\n",
    "\n",
    "    ## Test\n",
    "    preds = clf.predict_proba(X_test)\n",
    "    # test_auc = roc_auc_score(y_score=preds[:,1], y_true=y_test)\n",
    "    test_accuracy = accuracy_score(y_true=y_test, y_pred= np.argmax(preds, axis=1))\n",
    "    test_accs.append(round(test_accuracy*100, 1))\n",
    "    print(f\"TEST accuracy is {test_accuracy}\")\n",
    "\n",
    "    explain_matrix, masks = clf.explain(X_test, normalize=True)\n",
    "    weight = np.mean(explain_matrix, axis=0)\n",
    "    weights.append(weight)\n",
    "    \n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20,20))\n",
    "    for i in range(3):\n",
    "        axs[i].imshow(masks[i][:50])\n",
    "        axs[i].set_title(f\"mask {i}\")\n",
    "    fig.savefig(\"masks.png\")\n",
    "\n",
    "    # plot explain matrix\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(explain_matrix)\n",
    "    plt.title(\"explain_matrix\")\n",
    "    plt.savefig(save_dir + \"explain_matrix.png\")\n",
    "\n",
    "    # plot losses\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(clf.history['loss'], label='Train loss')\n",
    "    plt.title(\"Loss over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_dir +  \"loss_plot.png\")\n",
    "\n",
    "    # plot auc\n",
    "    plt.figure(figsize=(10,6))  # Create a new figure\n",
    "    plt.plot(clf.history['train_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(clf.history['valid_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(\"Accuracy over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(save_dir + \"accuracy_plot.png\")\n",
    "\n",
    "    # plot learning rates\n",
    "    plt.figure(figsize=(10,6))  # Create another new figure\n",
    "    plt.plot(clf.history['lr'])\n",
    "    plt.title(\"Learning Rate over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "    plt.savefig(save_dir + \"learning_rate_plot.png\")\n",
    "\n",
    "print(test_accs)\n",
    "print(f\"weights are {np.mean(weights, axis=0)}\")\n",
    "print(f\"MEAN is {sum(test_accs)/len(test_accs)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
